# -*- coding: utf-8 -*-
"""air_quality_pipeline_dag

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11DLeeMuX1BRNAQWXVUrhD8jzas6MRsxq
"""

from pathlib import Path

# DAG 파일 내용
dag_code = """
from airflow import DAG
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
import requests
import pandas as pd
from google.cloud import storage

# DAG 기본 설정
default_args = {
    'start_date': days_ago(1),
    'retries': 1,
}

dag = DAG(
    'air_quality_pipeline_dag',
    default_args=default_args,
    description='Airflow DAG for real-time air quality monitoring',
    schedule_interval='@hourly',
    catchup=False,
)

# 환경 변수
PROJECT_ID = 'crested-bonfire-464501-k8'
BUCKET_NAME = 'us-central1-airflow-final-d5ea4b16-bucket'
DATASET_NAME = 'air_quality_dataset'
TABLE_NAME = 'air_quality'
SERVICE_KEY = "9Ds02ZptFyRbU7aeLMcyV9KDjW4/YM2TGt1Yo9mgu76ZPCV8wTkJl8poRvjwPX4gvrCDkTqS1GdrsiqLLrXMog=="
API_URL = 'http://apis.data.go.kr/B552584/ArpltnInforInqireSvc/getCtprvnRltmMesureDnsty'

# 시도 목록
SIDO_LIST = [
    '서울', '부산', '대구', '인천', '광주', '대전', '울산',
    '세종', '경기', '강원', '충북', '충남', '전북', '전남',
    '경북', '경남', '제주'
]

# API 데이터 수집 함수
def fetch_air_quality_data():
    records = []
    for sido in SIDO_LIST:
        params = {
            'serviceKey': SERVICE_KEY,
            'returnType': 'xml',
            'numOfRows': '100',
            'pageNo': '1',
            'sidoName': sido,
            'ver': '1.0'
        }
        response = requests.get(API_URL, params=params)
        df = pd.read_xml(response.content, xpath='.//item')
        if df is not None:
            df['sidoName'] = sido  # 시도 이름 추가
            records.append(df)
    result_df = pd.concat(records, ignore_index=True)
    result_df.to_csv('/tmp/air_quality.csv', index=False)

    # GCS 업로드
    storage_client = storage.Client()
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob('air_quality/air_quality.csv')
    blob.upload_from_filename('/tmp/air_quality.csv')

# GCS 버킷 생성 (없으면)
create_bucket = GCSCreateBucketOperator(
    task_id='create_bucket',
    bucket_name=BUCKET_NAME,
    location='US',
    project_id=PROJECT_ID,
    dag=dag,
)

# API 데이터 수집 및 GCS 업로드
fetch_data_task = PythonOperator(
    task_id='fetch_air_quality_data',
    python_callable=fetch_air_quality_data,
    dag=dag,
)

# GCS → BigQuery 로드
load_to_bq = GCSToBigQueryOperator(
    task_id='load_to_bigquery',
    bucket=BUCKET_NAME,
    source_objects=['air_quality/air_quality.csv'],
    destination_project_dataset_table=f'{PROJECT_ID}.{DATASET_NAME}.{TABLE_NAME}',
    schema_fields=[
        {'name': 'dataTime', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'sidoName', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'stationName', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'khaiValue', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'pm10Value', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'pm25Value', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'so2Value', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'coValue', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'o3Value', 'type': 'STRING', 'mode': 'NULLABLE'},
        {'name': 'no2Value', 'type': 'STRING', 'mode': 'NULLABLE'},
    ],
    source_format='CSV',
    skip_leading_rows=1,
    write_disposition='WRITE_TRUNCATE',
    autodetect=True,
    dag=dag,
)

# View 생성 쿼리
create_views = BigQueryInsertJobOperator(
    task_id='create_views',
    configuration={
        "query": {
            "query": """
            CREATE OR REPLACE VIEW `{project}.{dataset}.kpi_summary_view` AS
            SELECT
              COUNT(DISTINCT stationName) AS total_stations,
              ROUND(AVG(SAFE_CAST(NULLIF(khaiValue, '-') AS FLOAT64)), 2) AS avg_khai,
              SUM(CASE WHEN SAFE_CAST(khaiValue AS FLOAT64) >= 151 THEN 1 ELSE 0 END) AS alarm_count
            FROM `{project}.{dataset}.{table}`;

            CREATE OR REPLACE VIEW `{project}.{dataset}.city_level_air_quality_view` AS
            SELECT
              sidoName,
              ROUND(AVG(SAFE_CAST(NULLIF(khaiValue, '-') AS FLOAT64)), 2) AS avg_khai,
              MAX(TIMESTAMP(dataTime)) AS last_updated
            FROM `{project}.{dataset}.{table}`
            WHERE SAFE_CAST(khaiValue AS FLOAT64) IS NOT NULL
            GROUP BY sidoName;

            CREATE OR REPLACE VIEW `{project}.{dataset}.station_trend_view` AS
            SELECT
              stationName,
              TIMESTAMP(dataTime) AS measurement_time,
              SAFE_CAST(pm10Value AS FLOAT64) AS pm10,
              SAFE_CAST(pm25Value AS FLOAT64) AS pm25,
              SAFE_CAST(so2Value AS FLOAT64) AS so2,
              SAFE_CAST(coValue AS FLOAT64) AS co,
              SAFE_CAST(o3Value AS FLOAT64) AS o3,
              SAFE_CAST(no2Value AS FLOAT64) AS no2,
              SAFE_CAST(khaiValue AS FLOAT64) AS khai
            FROM `{project}.{dataset}.{table}`
            WHERE SAFE_CAST(khaiValue AS FLOAT64) IS NOT NULL;
            """.format(project=PROJECT_ID, dataset=DATASET_NAME, table=TABLE_NAME),
            "useLegacySql": False,
        }
    },
    dag=dag,
)

# DAG 작업 순서 지정
create_bucket >> fetch_data_task >> load_to_bq >> create_views
"""

# 저장
dag_file = Path("/mnt/data/air_quality_pipeline_dag.py")
dag_file.write_text(dag_code)

dag_file.name